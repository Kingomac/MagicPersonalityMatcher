{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qdVVlOGGWNX"
   },
   "source": [
    "# Magic Personality Matcher - Preprocesamiento de datos\n",
    "\n",
    "Para evitar tener que preprocesar los datos cada vez que abrimos el Google Colab vamos a realizar tanto particionado de los datos como su preprocesamiento en este notebook aparte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Cargar el dataset\n",
    "\n",
    "Lo primero es cargar el dataset con Pandas y seleccionar la cantidad de filas que queremos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a2l03s6PVk",
    "outputId": "94476243-85aa-46e5-805a-94eac9fbc952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general counts\n",
      "personality\n",
      "INTP    27076\n",
      "INTJ    24299\n",
      "INFJ    17490\n",
      "INFP    15248\n",
      "ENTP    12987\n",
      "ENFP     7571\n",
      "ISTP     4088\n",
      "ENTJ     3465\n",
      "ENFJ     2242\n",
      "ESTP     2175\n",
      "ISTJ     1707\n",
      "ISFP     1513\n",
      "ISFJ     1180\n",
      "ESTJ      602\n",
      "ESFP      582\n",
      "ESFJ      328\n",
      "Name: count, dtype: int64\n",
      "used dataset info\n",
      "(122553, 2)\n",
      "  personality                                               post\n",
      "0        INTJ  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...\n",
      "1        INTJ  @Hispanthicckk Being you makes you look cute||...\n",
      "2        INTJ  @Alshymi Les balles sont réelles et sont tirée...\n",
      "3        INTJ  I'm like entp but idiotic|||Hey boy, do you wa...\n",
      "4        INTJ  @kaeshurr1 Give it to @ZargarShanif ... He has...\n",
      "personality\n",
      "INTP    27076\n",
      "INTJ    24299\n",
      "INFJ    17490\n",
      "INFP    15248\n",
      "ENTP    12987\n",
      "ENFP     7571\n",
      "ISTP     4088\n",
      "ENTJ     3465\n",
      "ENFJ     2242\n",
      "ESTP     2175\n",
      "ISTJ     1707\n",
      "ISFP     1513\n",
      "ISFJ     1180\n",
      "ESTJ      602\n",
      "ESFP      582\n",
      "ESFJ      328\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"dataset_definitivo.csv\")\n",
    "print(\"general counts\")\n",
    "print(raw_data['personality'].value_counts())\n",
    "training_data = raw_data#.head(5000)\n",
    "\n",
    "print(\"used dataset info\")\n",
    "print(training_data.shape)\n",
    "print(training_data.head())\n",
    "print(training_data['personality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Filtrado de Stopwords, Tokenización y Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpqMuv9eMDco"
   },
   "source": [
    "El preprocesamiento de texto se realiza usando las stopwords y el tokenizer de la librería de lenguaje natural de Python. Primero definimos las stopwords, que serán las de la librería de Python y las personalidades Myers-Briggs para evitar introducir sesgos, como que, si se menciona una personalidad, que esta no se tenga en cuenta para decidir la del autor. Luego se usa el PorterStemmer para obtener la “raíz” de las palabras. Entonces el resultado es una nueva columna que contiene los posts preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "zpL_KohnCtQd",
    "outputId": "ea21a2ca-0eab-45a5-9841-b94a6db658c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/tmp/ipykernel_8206/860219597.py:31: TqdmWarning: Iterable length 122553 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  preprocessedData['processed_text'] = process_map(process_text, training_data['post'], max_workers=num_processes)\n",
      " 10%|███▋                               | 12734/122553 [00:44<04:38, 393.89it/s]"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "#from tkinter.constants import W\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "STOPS = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOPS=set(['istj', 'isfj', 'infj', 'intj', 'istp', 'isfp', 'infp', 'intp', 'estp', 'esfp', 'enfp', 'entp', 'estj', 'esfj', 'enfj', 'entj'])\n",
    "\n",
    "def process_text(post):\n",
    "    post = post.lower()\n",
    "    post = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',post)\n",
    "    post = re.sub('[^0-9a-z]',' ',post)\n",
    "    return \" \".join([ps.stem(w) for w in word_tokenize(post) if not w in STOPS and w not in CUSTOM_STOPS])\n",
    "\n",
    "preprocessedData = training_data.loc[:]\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "try:\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        preprocessedData['processed_text'] = process_map(process_text, training_data['post'], max_workers=num_processes)\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "preprocessedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Particionar el dataset para entrenamiento y validación\n",
    "Se divide el dataset en 2 para tener 80% datos de entrenamiento y 20% de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessedData['processed_text']\n",
    "Y = preprocessedData['personality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGpqAmK4MHJl"
   },
   "source": [
    "## Fase 4 - Bolsa de palabras\n",
    "\n",
    "Se crea una bolsa de palabras utilizando TfidVectorizer, que se basa en la frecuencia de las palabras para determinar su importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZdzH4i6TEL2",
    "outputId": "d1d9174f-80a1-40f8-e12e-d5364b3eaba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train bag of words:\n",
      "(4000, 207345)\n",
      "X_test bag of words:\n",
      "(1000, 76462)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Datos de entrenamiento\n",
    "bagOfWordsModel = TfidfVectorizer()\n",
    "bagOfWordsModel.fit(X_train)\n",
    "train_textsBoW = bagOfWordsModel.transform(X_train)\n",
    "print(\"X_train bag of words:\")\n",
    "print(train_textsBoW.shape)\n",
    "\n",
    "# Datos pruebas\n",
    "bagOfWordsModel = TfidfVectorizer()\n",
    "bagOfWordsModel.fit(X_test)\n",
    "test_textsBoW = bagOfWordsModel.transform(X_test)\n",
    "print(\"X_test bag of words:\")\n",
    "print(test_textsBoW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOyc2LmHMZ5p"
   },
   "source": [
    "## Fase 5 - Guardar el resultado\n",
    "\n",
    "Utilizamos joblib para guardar estas bolsas de palabras para poder usarlas para entrenamiento sin tener que repetir el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4j6m5N1E1Cs",
    "outputId": "bac883e4-66d3-49c1-b0ac-925a587c80e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_textsBoW.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(train_textsBoW, \"train_textsBoW.joblib\")\n",
    "dump(test_textsBoW, \"test_textsBoW.joblib\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
