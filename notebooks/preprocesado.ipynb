{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qdVVlOGGWNX"
   },
   "source": [
    "# Preprocesamiento de datos\n",
    "\n",
    "Para evitar tener que preprocesar los datos cada vez que abrimos el Google Colab vamos a realizar tanto particionado de los datos como su preprocesamiento en este notebook aparte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Cargar el dataset\n",
    "\n",
    "Lo primero es cargar el dataset con Pandas y seleccionar la cantidad de filas que queremos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a2l03s6PVk",
    "outputId": "94476243-85aa-46e5-805a-94eac9fbc952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general counts\n",
      "personality\n",
      "7     257836\n",
      "3     218768\n",
      "8     188915\n",
      "4     176474\n",
      "11    133049\n",
      "12    121199\n",
      "15     78908\n",
      "5      61032\n",
      "6      59402\n",
      "2      55199\n",
      "16     50715\n",
      "1      44130\n",
      "10     25463\n",
      "9      18853\n",
      "14     15633\n",
      "13     13672\n",
      "Name: count, dtype: int64\n",
      "used dataset info\n",
      "(10000, 2)\n",
      "   personality                                               post\n",
      "0            4  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...\n",
      "1            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil A...\n",
      "2            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...\n",
      "3            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...\n",
      "4            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil T...\n",
      "personality\n",
      "4     4765\n",
      "8     4064\n",
      "16    1171\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"dataset_definitivo.csv\")\n",
    "print(\"general counts\")\n",
    "print(raw_data['personality'].value_counts())\n",
    "training_data = raw_data[:10000]\n",
    "\n",
    "print(\"used dataset info\")\n",
    "print(training_data.shape)\n",
    "print(training_data.head())\n",
    "print(training_data['personality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Filtrado de Stopwords, Tokenización y Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpqMuv9eMDco"
   },
   "source": [
    "El preprocesamiento de texto se realiza usando las stopwords y el tokenizer de la librería de lenguaje natural de Python. Primero definimos las stopwords, que serán las de la librería de Python y las personalidades Myers-Briggs para evitar introducir sesgos, como que, si se menciona una personalidad, que esta no se tenga en cuenta para decidir la del autor. Luego se usa el PorterStemmer para obtener la “raíz” de las palabras. Entonces el resultado es una nueva columna que contiene los posts preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "zpL_KohnCtQd",
    "outputId": "ea21a2ca-0eab-45a5-9841-b94a6db658c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a96e11b0467455bbfa776ed486fe3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personality</th>\n",
       "      <th>post</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@Pericles216 @HierBeforeTheAC @Sachinettiyil T...</td>\n",
       "      <td>pericles216 hierbeforetheac sachinettiyil pope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil A...</td>\n",
       "      <td>hierbeforetheac pericles216 sachinettiyil perp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil Y...</td>\n",
       "      <td>hierbeforetheac pericles216 sachinettiyil open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil Y...</td>\n",
       "      <td>hierbeforetheac pericles216 sachinettiyil know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil T...</td>\n",
       "      <td>hierbeforetheac pericles216 sachinettiyil like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>16</td>\n",
       "      <td>@threadreaderapp unroll</td>\n",
       "      <td>threadreaderapp unrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>16</td>\n",
       "      <td>In my personal life being an #empath #HSP mean...</td>\n",
       "      <td>person life empath hsp mean care friend famili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>16</td>\n",
       "      <td>Being a #HSP #empath means in my professional ...</td>\n",
       "      <td>hsp empath mean profession life person capabl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>16</td>\n",
       "      <td>But more and more over the past years I have c...</td>\n",
       "      <td>past year chang longer afraid tell peopl sensit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>16</td>\n",
       "      <td>But being #empathetic #HSP takes a toll. It’s ...</td>\n",
       "      <td>empathet hsp take toll stress pick carri peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      personality                                               post  \\\n",
       "0               4  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...   \n",
       "1               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil A...   \n",
       "2               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...   \n",
       "3               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...   \n",
       "4               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil T...   \n",
       "...           ...                                                ...   \n",
       "9995           16                            @threadreaderapp unroll   \n",
       "9996           16  In my personal life being an #empath #HSP mean...   \n",
       "9997           16  Being a #HSP #empath means in my professional ...   \n",
       "9998           16  But more and more over the past years I have c...   \n",
       "9999           16  But being #empathetic #HSP takes a toll. It’s ...   \n",
       "\n",
       "                                         processed_text  \n",
       "0     pericles216 hierbeforetheac sachinettiyil pope...  \n",
       "1     hierbeforetheac pericles216 sachinettiyil perp...  \n",
       "2     hierbeforetheac pericles216 sachinettiyil open...  \n",
       "3     hierbeforetheac pericles216 sachinettiyil know...  \n",
       "4     hierbeforetheac pericles216 sachinettiyil like...  \n",
       "...                                                 ...  \n",
       "9995                              threadreaderapp unrol  \n",
       "9996  person life empath hsp mean care friend famili...  \n",
       "9997  hsp empath mean profession life person capabl ...  \n",
       "9998    past year chang longer afraid tell peopl sensit  \n",
       "9999  empathet hsp take toll stress pick carri peopl...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "STOPS = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOPS = set(['istj', 'isfj', 'infj', 'intj',\n",
    "                    'istp', 'isfp', 'infp', 'intp',\n",
    "                    'estp', 'esfp', 'enfp', 'entp',\n",
    "                    'estj', 'esfj', 'enfj', 'entj'])\n",
    "\n",
    "def process_text(post):\n",
    "    try:\n",
    "        post = post.lower()\n",
    "        post = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',post)\n",
    "        post = re.sub('[^0-9a-z]',' ',post)\n",
    "        return \" \".join(\n",
    "            [ps.stem(w) for w in word_tokenize(post)\n",
    "             if not w in STOPS and w not in CUSTOM_STOPS])\n",
    "    except:\n",
    "        print(\"problem with: \", post)\n",
    "\n",
    "preprocessedData = training_data.loc[:]\n",
    "\n",
    "num_processes = cpu_count()\n",
    "\n",
    "preprocessedData['processed_text'] = process_map(\n",
    "                    process_text, training_data['post'],\n",
    "                    max_workers=num_processes, chunksize=10)\n",
    "\n",
    "preprocessedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Particionar el dataset para entrenamiento y validación\n",
    "Se divide el dataset en 2 para tener 80% datos de entrenamiento y 20% de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessedData['processed_text']\n",
    "Y = preprocessedData['personality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGpqAmK4MHJl"
   },
   "source": [
    "## Fase 4 - Bolsa de palabras\n",
    "\n",
    "Se crea una bolsa de palabras utilizando TfidVectorizer, que se basa en la frecuencia de las palabras para determinar su importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZdzH4i6TEL2",
    "outputId": "d1d9174f-80a1-40f8-e12e-d5364b3eaba6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train bag of words:\n",
      "(8000, 12043)\n",
      "X_test bag of words:\n",
      "(2000, 12043)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Datos de entrenamiento\n",
    "bagOfWordsModel = TfidfVectorizer()\n",
    "X_train = bagOfWordsModel.fit_transform(X_train)\n",
    "print(\"X_train bag of words:\")\n",
    "print(X_train.shape)\n",
    "\n",
    "# Datos pruebas\n",
    "X_test = bagOfWordsModel.transform(X_test)\n",
    "print(\"X_test bag of words:\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos TruncatedSVD para reducir la dimensionalidad de los datos de entrenamiento y prueba, permitiendo así trabajar con un número menor de características manteniendo la información relevante para el modelo. Esto puede ser útil en casos donde se tiene un alto número de características y se desea reducir la complejidad del modelo o mejorar los tiempos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD()\n",
    "X_train = svd.fit_transform(X_train)\n",
    "X_test = svd.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOyc2LmHMZ5p"
   },
   "source": [
    "## Fase 5 - Guardar el resultado\n",
    "\n",
    "Utilizamos joblib para guardar estas bolsas de palabras para poder usarlas para entrenamiento sin tener que repetir el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4j6m5N1E1Cs",
    "outputId": "bac883e4-66d3-49c1-b0ac-925a587c80e3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4be758d471d4ffdaf118169a69dcc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import process_map,thread_map\n",
    "from joblib import dump\n",
    "\n",
    "def multiargs_dump(args_tuple):\n",
    "    dump(args_tuple[0], args_tuple[1])\n",
    "\n",
    "\n",
    "thread_map(multiargs_dump,\n",
    "    (\n",
    "        (X_train, \"X_train.lzma\"),\n",
    "        (X_test, \"X_test.lzma\"),\n",
    "        (y_train, \"y_train.lzma\"),\n",
    "        (y_test, \"y_test.lzma\"),\n",
    "        (bagOfWordsModel, \"bag_of_words.lzma\")\n",
    "    ))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
