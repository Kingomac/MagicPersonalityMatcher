{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qdVVlOGGWNX"
   },
   "source": [
    "# Magic Personality Matcher - Preprocesamiento de datos\n",
    "\n",
    "Para evitar tener que preprocesar los datos cada vez que abrimos el Google Colab vamos a realizar tanto particionado de los datos como su preprocesamiento en este notebook aparte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Cargar el dataset\n",
    "\n",
    "Lo primero es cargar el dataset con Pandas y seleccionar la cantidad de filas que queremos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a2l03s6PVk",
    "outputId": "94476243-85aa-46e5-805a-94eac9fbc952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general counts\n",
      "personality\n",
      "INTP    27076\n",
      "INTJ    24299\n",
      "INFJ    17490\n",
      "INFP    15248\n",
      "ENTP    12987\n",
      "ENFP     7571\n",
      "ISTP     4088\n",
      "ENTJ     3465\n",
      "ENFJ     2242\n",
      "ESTP     2175\n",
      "ISTJ     1707\n",
      "ISFP     1513\n",
      "ISFJ     1180\n",
      "ESTJ      602\n",
      "ESFP      582\n",
      "ESFJ      328\n",
      "Name: count, dtype: int64\n",
      "used dataset info\n",
      "(5000, 2)\n",
      "  personality                                               post\n",
      "0        INTJ  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...\n",
      "1        INTJ  @Hispanthicckk Being you makes you look cute||...\n",
      "2        INTJ  @Alshymi Les balles sont réelles et sont tirée...\n",
      "3        INTJ  I'm like entp but idiotic|||Hey boy, do you wa...\n",
      "4        INTJ  @kaeshurr1 Give it to @ZargarShanif ... He has...\n",
      "personality\n",
      "INFP    811\n",
      "INFJ    628\n",
      "INTP    556\n",
      "INTJ    456\n",
      "ENFP    429\n",
      "ENTP    370\n",
      "ENFJ    330\n",
      "ISFJ    259\n",
      "ISFP    229\n",
      "ISTP    224\n",
      "ISTJ    204\n",
      "ENTJ    185\n",
      "ESFP    119\n",
      "ESFJ     73\n",
      "ESTP     67\n",
      "ESTJ     60\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"dataset_definitivo.csv\")\n",
    "print(\"general counts\")\n",
    "print(raw_data['personality'].value_counts())\n",
    "training_data = raw_data.head(5000)\n",
    "\n",
    "print(\"used dataset info\")\n",
    "print(training_data.shape)\n",
    "print(training_data.head())\n",
    "print(training_data['personality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Filtrado de Stopwords, Tokenización y Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpqMuv9eMDco"
   },
   "source": [
    "El preprocesamiento de texto se realiza usando las stopwords y el tokenizer de la librería de lenguaje natural de Python. Primero definimos las stopwords, que serán las de la librería de Python y las personalidades Myers-Briggs para evitar introducir sesgos, como que, si se menciona una personalidad, que esta no se tenga en cuenta para decidir la del autor. Luego se usa el PorterStemmer para obtener la “raíz” de las palabras. Entonces el resultado es una nueva columna que contiene los posts preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "zpL_KohnCtQd",
    "outputId": "ea21a2ca-0eab-45a5-9841-b94a6db658c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3344ea2acbab46a0ba5d6668272573a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personality</th>\n",
       "      <th>post</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>@Pericles216 @HierBeforeTheAC @Sachinettiyil T...</td>\n",
       "      <td>pericles216 hierbeforetheac sachinettiyil pope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>@Hispanthicckk Being you makes you look cute||...</td>\n",
       "      <td>hispanthicckk make look cute thiccwhiteduk fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>@Alshymi Les balles sont réelles et sont tirée...</td>\n",
       "      <td>alshymi le ball sont r ell et sont tir es tr r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>I'm like entp but idiotic|||Hey boy, do you wa...</td>\n",
       "      <td>like idiot hey boy want watch twitch kin simon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>@kaeshurr1 Give it to @ZargarShanif ... He has...</td>\n",
       "      <td>kaeshurr1 give zargarshanif pica sinc childhoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>@TheChoirLoft https://t.co/LJjH1v5sKV|||@allie...</td>\n",
       "      <td>thechoirloft got depress feel like lot peopl w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>@danrannosaurus Got it. Thank youuu!! Haha|||@...</td>\n",
       "      <td>danrannosauru got thank youuu haha danrannosau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>INTP</td>\n",
       "      <td>WHY☹️☹️☹️☹️ https://t.co/DwJRFJyz6Z|||WHY IS H...</td>\n",
       "      <td>cryinf soop friendcat us disney lucki 4 got ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>INTP</td>\n",
       "      <td>@sashywaybrights I LOVE THAT SONG OMG|||@ricep...</td>\n",
       "      <td>sashywaybright love song omg ricepoptart marci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>INTP</td>\n",
       "      <td>@youlbsavia exercise|||@skreamzgore goodnight ...</td>\n",
       "      <td>youlbsavia exercis skreamzgor goodnight lu wai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     personality                                               post  \\\n",
       "0           INTJ  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...   \n",
       "1           INTJ  @Hispanthicckk Being you makes you look cute||...   \n",
       "2           INTJ  @Alshymi Les balles sont réelles et sont tirée...   \n",
       "3           INTJ  I'm like entp but idiotic|||Hey boy, do you wa...   \n",
       "4           INTJ  @kaeshurr1 Give it to @ZargarShanif ... He has...   \n",
       "...          ...                                                ...   \n",
       "4995        INTJ  @TheChoirLoft https://t.co/LJjH1v5sKV|||@allie...   \n",
       "4996        INTJ  @danrannosaurus Got it. Thank youuu!! Haha|||@...   \n",
       "4997        INTP  WHY☹️☹️☹️☹️ https://t.co/DwJRFJyz6Z|||WHY IS H...   \n",
       "4998        INTP  @sashywaybrights I LOVE THAT SONG OMG|||@ricep...   \n",
       "4999        INTP  @youlbsavia exercise|||@skreamzgore goodnight ...   \n",
       "\n",
       "                                         processed_text  \n",
       "0     pericles216 hierbeforetheac sachinettiyil pope...  \n",
       "1     hispanthicckk make look cute thiccwhiteduk fun...  \n",
       "2     alshymi le ball sont r ell et sont tir es tr r...  \n",
       "3     like idiot hey boy want watch twitch kin simon...  \n",
       "4     kaeshurr1 give zargarshanif pica sinc childhoo...  \n",
       "...                                                 ...  \n",
       "4995  thechoirloft got depress feel like lot peopl w...  \n",
       "4996  danrannosauru got thank youuu haha danrannosau...  \n",
       "4997  cryinf soop friendcat us disney lucki 4 got ah...  \n",
       "4998  sashywaybright love song omg ricepoptart marci...  \n",
       "4999  youlbsavia exercis skreamzgor goodnight lu wai...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "STOPS = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOPS=set(['istj', 'isfj', 'infj', 'intj', 'istp', 'isfp', 'infp', 'intp', 'estp', 'esfp', 'enfp', 'entp', 'estj', 'esfj', 'enfj', 'entj'])\n",
    "\n",
    "def process_text(post):\n",
    "    post = post.lower()\n",
    "    post = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',post)\n",
    "    post = re.sub('[^0-9a-z]',' ',post)\n",
    "    return \" \".join([ps.stem(w) for w in word_tokenize(post) if not w in STOPS and w not in CUSTOM_STOPS])\n",
    "\n",
    "preprocessedData = training_data.loc[:]\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "try:\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        preprocessedData['processed_text'] = process_map(process_text, training_data['post'], max_workers=num_processes, chunksize=10)\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "preprocessedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Particionar el dataset para entrenamiento y validación\n",
    "Se divide el dataset en 2 para tener 80% datos de entrenamiento y 20% de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessedData['processed_text']\n",
    "Y = preprocessedData['personality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGpqAmK4MHJl"
   },
   "source": [
    "## Fase 4 - Bolsa de palabras\n",
    "\n",
    "Se crea una bolsa de palabras utilizando TfidVectorizer, que se basa en la frecuencia de las palabras para determinar su importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZdzH4i6TEL2",
    "outputId": "d1d9174f-80a1-40f8-e12e-d5364b3eaba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train bag of words:\n",
      "(4000, 207345)\n",
      "X_test bag of words:\n",
      "(1000, 207345)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Datos de entrenamiento\n",
    "bagOfWordsModel = TfidfVectorizer()\n",
    "train_textsBoW = bagOfWordsModel.fit_transform(X_train)\n",
    "print(\"X_train bag of words:\")\n",
    "print(train_textsBoW.shape)\n",
    "\n",
    "# Datos pruebas\n",
    "test_textsBoW = bagOfWordsModel.transform(X_test)\n",
    "print(\"X_test bag of words:\")\n",
    "print(test_textsBoW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOyc2LmHMZ5p"
   },
   "source": [
    "## Fase 5 - Guardar el resultado\n",
    "\n",
    "Utilizamos joblib para guardar estas bolsas de palabras para poder usarlas para entrenamiento sin tener que repetir el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4j6m5N1E1Cs",
    "outputId": "bac883e4-66d3-49c1-b0ac-925a587c80e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bag_of_words.lzma']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "import numpy as np\n",
    "\n",
    "dump(train_textsBoW, \"train_textsBoW.lzma\")\n",
    "dump(test_textsBoW, \"test_textsBoW.lzma\")\n",
    "dump(y_train, \"y_train.lzma\")\n",
    "dump(y_test, \"y_test.lzma\")\n",
    "dump(bagOfWordsModel, \"bag_of_words.lzma\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
