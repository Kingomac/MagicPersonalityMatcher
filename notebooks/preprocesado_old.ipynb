{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qdVVlOGGWNX"
   },
   "source": [
    "# Preprocesamiento de datos\n",
    "\n",
    "Para evitar tener que preprocesar los datos cada vez que abrimos el Google Colab vamos a realizar tanto particionado de los datos como su preprocesamiento en este notebook aparte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Cargar el dataset\n",
    "\n",
    "Lo primero es cargar el dataset con Pandas y seleccionar la cantidad de filas que queremos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a2l03s6PVk",
    "outputId": "94476243-85aa-46e5-805a-94eac9fbc952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general counts\n",
      "personality\n",
      "7     271107\n",
      "3     227906\n",
      "8     198143\n",
      "4     183628\n",
      "11    139942\n",
      "12    128444\n",
      "15     83261\n",
      "5      65455\n",
      "6      63511\n",
      "2      58852\n",
      "16     53388\n",
      "1      46791\n",
      "10     27698\n",
      "9      19894\n",
      "14     16627\n",
      "13     14381\n",
      "Name: count, dtype: int64\n",
      "used dataset info\n",
      "(10000, 2)\n",
      "   personality                                               post\n",
      "0            4  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...\n",
      "1            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil A...\n",
      "2            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...\n",
      "3            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...\n",
      "4            4  @HierBeforeTheAC @Pericles216 @Sachinettiyil T...\n",
      "personality\n",
      "4     4972\n",
      "8     4316\n",
      "16     712\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"dataset_definitivo.csv\")\n",
    "print(\"general counts\")\n",
    "print(raw_data['personality'].value_counts())\n",
    "training_data = raw_data[:10000]\n",
    "\n",
    "print(\"used dataset info\")\n",
    "print(training_data.shape)\n",
    "print(training_data.head())\n",
    "print(training_data['personality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Filtrado de Stopwords, Tokenización y Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpqMuv9eMDco"
   },
   "source": [
    "El preprocesamiento de texto se realiza usando las stopwords y el tokenizer de la librería de lenguaje natural de Python. Primero definimos las stopwords, que serán las de la librería de Python y las personalidades Myers-Briggs para evitar introducir sesgos, como que, si se menciona una personalidad, que esta no se tenga en cuenta para decidir la del autor. Luego se usa el PorterStemmer para obtener la “raíz” de las palabras. Entonces el resultado es una nueva columna que contiene los posts preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "zpL_KohnCtQd",
    "outputId": "ea21a2ca-0eab-45a5-9841-b94a6db658c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b43f5079b774807981e6adc93795d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personality</th>\n",
       "      <th>post</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@Pericles216 @HierBeforeTheAC @Sachinettiyil T...</td>\n",
       "      <td>pope infal cathol dogma mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil A...</td>\n",
       "      <td>perpetu entail church elect new po</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil Y...</td>\n",
       "      <td>open door uniron nonsens believ nonsens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil Y...</td>\n",
       "      <td>know faith lol tri say perpetua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@HierBeforeTheAC @Pericles216 @Sachinettiyil T...</td>\n",
       "      <td>like say gon na give bike never b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>16</td>\n",
       "      <td>Welcome to poverty. https://t.co/exnhGEKM4F</td>\n",
       "      <td>welcom poverti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>16</td>\n",
       "      <td>Democrats love mail in ballots. https://t.co/t...</td>\n",
       "      <td>democrat love mail ballot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>16</td>\n",
       "      <td>I miss training to be a boxer. No one ever exp...</td>\n",
       "      <td>miss train boxer one ever expect kid see come ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>16</td>\n",
       "      <td>I spend about 500$ a week on hotels. https://t...</td>\n",
       "      <td>spend week hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>16</td>\n",
       "      <td>I’ve become a great father over the past few y...</td>\n",
       "      <td>becom great father past year one thing say tak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      personality                                               post  \\\n",
       "0               4  @Pericles216 @HierBeforeTheAC @Sachinettiyil T...   \n",
       "1               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil A...   \n",
       "2               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...   \n",
       "3               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil Y...   \n",
       "4               4  @HierBeforeTheAC @Pericles216 @Sachinettiyil T...   \n",
       "...           ...                                                ...   \n",
       "9995           16        Welcome to poverty. https://t.co/exnhGEKM4F   \n",
       "9996           16  Democrats love mail in ballots. https://t.co/t...   \n",
       "9997           16  I miss training to be a boxer. No one ever exp...   \n",
       "9998           16  I spend about 500$ a week on hotels. https://t...   \n",
       "9999           16  I’ve become a great father over the past few y...   \n",
       "\n",
       "                                         processed_text  \n",
       "0                          pope infal cathol dogma mean  \n",
       "1                    perpetu entail church elect new po  \n",
       "2               open door uniron nonsens believ nonsens  \n",
       "3                       know faith lol tri say perpetua  \n",
       "4                     like say gon na give bike never b  \n",
       "...                                                 ...  \n",
       "9995                                     welcom poverti  \n",
       "9996                          democrat love mail ballot  \n",
       "9997  miss train boxer one ever expect kid see come ...  \n",
       "9998                                   spend week hotel  \n",
       "9999  becom great father past year one thing say tak...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "STOPS = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOPS = set(['istj', 'isfj', 'infj', 'intj',\n",
    "                    'istp', 'isfp', 'infp', 'intp',\n",
    "                    'estp', 'esfp', 'enfp', 'entp',\n",
    "                    'estj', 'esfj', 'enfj', 'entj'])\n",
    "\n",
    "def process_text(post):\n",
    "    try:\n",
    "        post = post.lower()\n",
    "        post = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',post) # filtrar links\n",
    "        post = re.sub('[0-9]+',' ',post) # filtrar numeros\n",
    "        post = re.sub('@[^\\s]+',' ',post) # filtrar menciones de Twitter\n",
    "        post = re.sub('[^0-9a-z]',' ',post) # filtrar caracteres especiais\n",
    "        post = re.sub('[a-z]{1,2,3}', ' ', post) # filtrar monogramas, bigramas y trigramas\n",
    "        return \" \".join(\n",
    "            [ps.stem(w) for w in word_tokenize(post)\n",
    "             if not w in STOPS and w not in CUSTOM_STOPS])\n",
    "    except:\n",
    "        print(\"problem with: \", post)\n",
    "\n",
    "preprocessedData = training_data.loc[:]\n",
    "\n",
    "num_processes = cpu_count()\n",
    "\n",
    "preprocessedData['processed_text'] = process_map(\n",
    "                    process_text, training_data['post'],\n",
    "                    max_workers=num_processes, chunksize=10)\n",
    "\n",
    "preprocessedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Particionar el dataset para entrenamiento y validación\n",
    "\n",
    "Se divide el dataset en 3 partes para tener 60% como datos de entrenamiento, 20% validación y 20% pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessedData['processed_text']\n",
    "Y = preprocessedData['personality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGpqAmK4MHJl"
   },
   "source": [
    "## Fase 4 - Bolsa de palabras\n",
    "\n",
    "Se crea una bolsa de palabras utilizando TfidVectorizer, que se basa en la frecuencia de las palabras para determinar su importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZdzH4i6TEL2",
    "outputId": "d1d9174f-80a1-40f8-e12e-d5364b3eaba6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train bag of words:\n",
      "(7500, 8470)\n",
      "X_val bag of words:\n",
      "(2500, 8470)\n",
      "X_test bag of words:\n",
      "(2000, 8470)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Datos de entrenamiento\n",
    "bagOfWordsModel = TfidfVectorizer()\n",
    "X_train = bagOfWordsModel.fit_transform(X_train)\n",
    "print(\"X_train bag of words:\")\n",
    "print(X_train.shape)\n",
    "\n",
    "# Datos de validación\n",
    "X_val = bagOfWordsModel.transform(X_val)\n",
    "print(\"X_val bag of words:\")\n",
    "print(X_val.shape)\n",
    "\n",
    "# Datos pruebas\n",
    "X_test = bagOfWordsModel.transform(X_test)\n",
    "print(\"X_test bag of words:\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 5 - Reducción de dimensionalidad\n",
    "\n",
    "Utilizamos TruncatedSVD para reducir la dimensionalidad de los datos de entrenamiento y prueba, permitiendo así trabajar con un número menor de características manteniendo la información relevante para el modelo. Esto puede ser útil en casos donde se tiene un alto número de características y se desea reducir la complejidad del modelo o mejorar los tiempos de entrenamiento.\n",
    "\n",
    "Para ello usamos la técnica de reducción de dimensionalidad Truncated SVD, se suele usar para NLP por permitir trabajar con matrices dispersas y grandes cantidades de datos más rápido.\n",
    "\n",
    "Este método tiene un problema denominado \"sign indeterminacy\", que explicado de forma sencilla, hace que los resultados entre sean inconsistentes usando los mismos parámetros y datos. Para evitarlo los desarrolladores recomiendan guardar la instancia inicial y usarla para procesar nuevas entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 1000)\n",
      "(2500, 1000)\n",
      "(2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, n_oversamples=15, random_state=42)\n",
    "X_train = svd.fit_transform(X_train)\n",
    "X_val = svd.transform(X_val)\n",
    "X_test = svd.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOyc2LmHMZ5p"
   },
   "source": [
    "## Fase 6 - Guardar el resultado\n",
    "\n",
    "Utilizamos joblib para guardar estas bolsas de palabras para poder usarlas para entrenamiento sin tener que repetir el proceso. Elegimos esta librería respecto a Pickle, que ya viene incluido en Python, porque permite comprimir el fichero resultante simplemente añadiendo una extensión de fichero comprimido al fichero de salida, así podemos enviar modelos preentrenados y datos de distintas etapas más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4j6m5N1E1Cs",
    "outputId": "bac883e4-66d3-49c1-b0ac-925a587c80e3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0d9d17fb13487eba5964b1b79873f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import process_map,thread_map\n",
    "from joblib import dump\n",
    "\n",
    "def multiargs_dump(args_tuple):\n",
    "    dump(args_tuple[0], args_tuple[1])\n",
    "\n",
    "\n",
    "thread_map(multiargs_dump,\n",
    "    (\n",
    "        (X_train, \"X_train.lzma\"),\n",
    "        (X_test, \"X_test.lzma\"),\n",
    "        (X_test, \"X_val.lzma\"),\n",
    "        (y_train, \"y_train.lzma\"),\n",
    "        (y_test, \"y_test.lzma\"),\n",
    "        (y_test, \"y_val.lzma\"),\n",
    "        (bagOfWordsModel, \"bag_of_words.lzma\"),\n",
    "        (svd, \"svd.lzma\")\n",
    "    ))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
