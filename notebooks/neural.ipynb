{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train bag of words:\n",
      "(8000, 8769)\n",
      "X_test bag of words:\n",
      "(2000, 8769)\n",
      "(8000, 2000)\n",
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "import joblib as jb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "preprocessedData = jb.load(\"preprocessed_data.lzma\")\n",
    "preprocessedData = preprocessedData.head(10_000)\n",
    "\n",
    "X = preprocessedData['processed_text']\n",
    "Y = preprocessedData['personality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Datos de entrenamiento\n",
    "bagOfWordsModel = TfidfVectorizer()\n",
    "X_train = bagOfWordsModel.fit_transform(X_train)\n",
    "print(\"X_train bag of words:\")\n",
    "print(X_train.shape)\n",
    "\n",
    "# Datos pruebas\n",
    "X_test = bagOfWordsModel.transform(X_test)\n",
    "print(\"X_test bag of words:\")\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "svd = TruncatedSVD(n_components=2000, n_oversamples=15, random_state=42)\n",
    "X_train = svd.fit_transform(X_train)\n",
    "X_test = svd.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8000, 2000)\n",
      "y_train.shape: (8000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preprocessedData, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_encoded, dtype=torch.long)  # Usar torch.long para clases múltiples\n",
    "\n",
    "input_size = 32\n",
    "hidden_size = 64\n",
    "\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(2000, 3000)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(3000, 2000)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden3 = nn.GRU(2000, 1000, batch_first=True)  # Corrección: especificar batch_first=True para la entrada\n",
    "        self.hidden4 = nn.RNN(1000, 1200, batch_first=True)  # Corrección: especificar batch_first=True para la entrada\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.hidden5 = nn.Linear(1200, 77)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.output = nn.Linear(77, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        \n",
    "        # Pasar a través de la capa GRU\n",
    "        x, _ = self.hidden3(x)\n",
    "        \n",
    "        # Pasar a través de la capa RNN\n",
    "        x, _ = self.hidden4(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.act5(self.hidden5(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU disponible\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Usando CPU\")\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "model = MyClassifier().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convertir y_train a torch.long si aún no lo está\n",
    "y_train = y_train.type(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mario/Documentos/MagicPersonalityMatcher/notebooks/neural.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mario/Documentos/MagicPersonalityMatcher/notebooks/neural.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(Xbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mario/Documentos/MagicPersonalityMatcher/notebooks/neural.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, ybatch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mario/Documentos/MagicPersonalityMatcher/notebooks/neural.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mario/Documentos/MagicPersonalityMatcher/notebooks/neural.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mario/Documentos/MagicPersonalityMatcher/notebooks/neural.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinished epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, latest loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        Xbatch = X_train[i:i+batch_size]\n",
    "        ybatch = y_train[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(Xbatch)\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00    1005.0\n",
      "           8       0.00      0.00      0.00     876.0\n",
      "          16       0.00      0.00      0.00     119.0\n",
      "\n",
      "    accuracy                           0.00    2000.0\n",
      "   macro avg       0.00      0.00      0.00    2000.0\n",
      "weighted avg       0.00      0.00      0.00    2000.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy (no_grad is optional)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_probs = model(X_test)\n",
    "    _, y_pred = torch.max(y_pred_probs, 1)\n",
    "\n",
    "y_pred = y_pred.numpy()\n",
    "y_true = y_test.numpy()\n",
    " \n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
